{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab4_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hExKCzh6doIW"
      },
      "source": [
        "# Lab 4 - Neural Network Classifier Using Simple Word Embeddings\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HixoFOoCIJ7V"
      },
      "source": [
        "In this session, we demonstrate how to solve a text classification task using simple \n",
        "feedforward neural network classifier. We will use IMDB Large Movie Review Dataset to train a binary classification model, able to predict whether a review is positive or negative. First, our network takes one-hot word vectors as input, averages them to make one vector and trains a \n",
        "fully-connected layer to predict the output. In the second part, we replace the one-hot vectors with the word embeddings and add a layer to see how much that improves the performance.\n",
        "\n",
        "We are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. But it is not straightforward to define models where layers connect to more than just the previous and next layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8fpBfhBpupy",
        "outputId": "f9730c95-8770-49af-fde6-463cd6f57ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EundMtGPpCdf"
      },
      "source": [
        "The dataset we will be using is the IMDB Large Movie Review Dataset, which consists of 50000 labeled movie reviews. These are split into 25,000 reviews for training and 25,000 reviews for testing. The  dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. The data is preprocessed. For text classification, it is ususal to limit the size of the vocabulary to stop the dataset from becoming too sparse, creating possible overfitting. We keep the top 10,000 most frequently occurring words in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NyuSzkafqNca",
        "colab": {}
      },
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, letâ€™s first see the length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h-gjWRAuqg5s",
        "outputId": "c7be7352-0339-4068-aebd-630415347b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(X_train), len(y_train)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 25000, labels: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MTRZrpcyr-4x"
      },
      "source": [
        "The  reviews have been converted to integers and each integer represents a  word in a dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "79Ev72Kgq4XL",
        "outputId": "eaca9f2e-4304-4597-b398-bd1e5c09abbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "X_train[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       list([1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 2, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 2, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 2, 594, 7, 5168, 94, 9096, 3987, 2, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 2, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 2, 19, 49, 7, 4, 1885, 2, 1118, 25, 80, 126, 842, 10, 10, 2, 2, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 2, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 2, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 2, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 2, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 2, 1775, 3353, 2, 1846, 4, 2, 7, 154, 5, 4, 518, 53, 2, 2, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 2, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 2, 9, 242, 4, 91, 1202, 2, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 2, 13, 188, 1076, 3222, 19, 4, 2, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 2, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 2, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 2, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 2, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574]),\n",
              "       list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113]),\n",
              "       list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]),\n",
              "       list([1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 2, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]),\n",
              "       list([1, 4, 2, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 2, 2, 5270, 2, 2315, 2, 2, 2, 2, 4, 2, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 2, 847, 313, 6, 176, 2, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 2, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 2, 6293, 345, 4804, 2, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 2, 5262, 2, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 2, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 2, 8, 783, 2, 33, 4, 2945, 103, 465, 2, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 2, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 2, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 2, 1703, 56, 8, 803, 1004, 6, 2, 155, 11, 4, 2, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 2, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 2, 367, 45, 115, 93, 788, 121, 4, 2, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 2, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 2, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 2, 2, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 2, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 2, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 2, 6847, 4, 2, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 2, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32]),\n",
              "       list([1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 2, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 2, 2, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 2, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 2, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110]),\n",
              "       list([1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 1109, 23, 94, 2, 151, 111, 211, 469, 4, 20, 13, 258, 546, 1104, 7273, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2849, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 1242, 1179, 20, 19, 6, 1009, 7, 1987, 189, 5, 6, 8419, 7, 2723, 2, 95, 1719, 6, 6035, 7, 3912, 7144, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 1041, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 1167, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y65HJaHc7YD",
        "colab_type": "code",
        "outputId": "17e75bc1-1ac2-417f-c39f-ef30a6440804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "We can convert integers back to words by querying a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gMCH1OoDrSNR",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5IreFXgruZot"
      },
      "source": [
        "Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown tokens. Index 0 will be used for padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abIb7Fe5u3GQ",
        "colab": {}
      },
      "source": [
        "\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<UNUSED>\"] = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "To reverse key and values in a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKOiVVXQu-_I",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZmTJEm8xvUvW"
      },
      "source": [
        "To view a word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SqN5jgVKvJJZ",
        "outputId": "ece932f1-c968-426f-ee4d-8e0b31e4ca93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reverse_word_index[25]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6QjrzgVvrYn"
      },
      "source": [
        "And to recreate the whole sentence from our training data we define decode_review:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvrKeMgxvWlv",
        "colab": {}
      },
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sxg4YA_NvdRg",
        "outputId": "e3cce171-4a29-4cb5-8d93-2ef15cb87b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "decode_review(X_train[10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c8gIzXncfaJK"
      },
      "source": [
        "### Creating One-hot word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9W4yb3rv_E0"
      },
      "source": [
        "It is  common to use one-hot representation as input in Natural Language Processing tasks. In Keras, the Embedding layer takes an index as an input and convert it to one-hot vector with the length of the vocabulary size. Then multiplies these vectors by a normal weight matrix. But there is no way to only get a one-hot vector as the output of a layer in Keras. To solve this we use Lambda() layer and a function that creates the one-hot layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RPO_pK9zH4C5",
        "colab": {}
      },
      "source": [
        "def OneHot(input_dim=None, input_length=None):\n",
        "    \n",
        "    if input_dim is None or input_length is None:\n",
        "        raise TypeError(\"input_dim or input_length is not set\")\n",
        "\n",
        "    \n",
        "    def _one_hot(x, num_classes):\n",
        "        return K.one_hot(K.cast(x, 'uint8'),\n",
        "                          num_classes=num_classes)\n",
        "\n",
        "    return Lambda(_one_hot,\n",
        "                  arguments={'num_classes': input_dim},\n",
        "                  input_shape=(input_length,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "364d3MAw0ez9"
      },
      "source": [
        "input_dim refers to the length of the one-hot vector and input_length refers to the length of the input sequence. Since the input to K.one_hot should be an integer tensor, we cast x to one (Keras passes around float tensors by default).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VHz76GNA2M4r"
      },
      "source": [
        " Each text sequence has in most cases different length of words. Here, we fill sequences with a pad token (0) to fit the size. This special tokens is then masked not to be accounted in averaging, loss calculation etc. We set the maximum length to 256."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9G_o7PsvgSFt"
      },
      "source": [
        "### Preparing input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jiFn7sd_wF5j",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "X_train_enc = keras.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=256)\n",
        "\n",
        "X_test_enc = keras.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kcjFH1wKF_7d"
      },
      "source": [
        "And to view a padded review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zwH4dcfW_a18",
        "outputId": "29573b0b-bfe1-44d3-b554-5e8eee71cfbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "print(X_train_enc[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
            "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
            "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
            "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
            "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
            "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
            "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
            "    7 8255    2  349 2637  148  605    2 8003   15  123  125   68    2\n",
            " 6853   15  349  165 4362   98    5    4  228    9   43    2 1157   15\n",
            "  299  120    5  120  174   11  220  175  136   50    9 4373  228 8255\n",
            "    5    2  656  245 2350    5    4 9837  131  152  491   18    2   32\n",
            " 7464 1212   14    9    6  371   78   22  625   64 1382    9    8  168\n",
            "  145   23    4 1690   15   16    4 1355    5   28    6   52  154  462\n",
            "   33   89   78  285   16  145   95    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1zcxFwNGepA"
      },
      "source": [
        "Now we want to build the neural network model. We  are going to have a hidden layer with 16 hidden units. \n",
        "\n",
        "First, we want to transform each index to an embedded vector and then average all vectors to a single one. It has been showed that unweighted average of word vectors outperforms many complicated networks that model semantic and syntactic compositionality. As an example you can take a look at this: (http://anthology.aclweb.org/P/P15/P15-1162.pdf)\n",
        "\n",
        "To average we need to ignore padded zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yi04MLIvJOGZ",
        "colab": {}
      },
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "whgIIB5ggjna"
      },
      "source": [
        "### Neural Network model using one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jlOLnlnSJgrU"
      },
      "source": [
        "The first layer is an one-hot layer. The second layer is to compute average on all word vectors in a sentence without considering padding. The  output vector is piped through a fully-connected layer. The last layer is connected with a single output node with the sigmoid activation function. The final value is a float between 0 and 1. \n",
        "The vocabulary count of the movie reviews (10000) is used as the input shape. At the end we visualize the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Pn83gBbxiK7",
        "outputId": "631c1b13-8783-4b6f-9f7c-39860d346968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "# put your code here\n",
        "model = Sequential()\n",
        "model.add(OneHot(VOCAB_SIZE,input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(GlobalAveragePooling1DMasked())\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 256, 10000)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Mz96xpCgvTj"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F3HbW_IKLqwT"
      },
      "source": [
        "To compile the model we need a loss function and an optimizer. We use binary_crossentropy loss function which is just a special case of categorical cross entropy. We also use Adam optimizer that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. You can read more about it here:\n",
        "(https://arxiv.org/abs/1412.6980v8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qh1PWTNMxjUw",
        "outputId": "61f97aec-c9ff-429b-f2d1-5a2b1cb05f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1jwQQqCN5Ia"
      },
      "source": [
        "When training, we want to check the accuracy of the model on data it hasn't seen before. So we create a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5lAqzQlxjSM",
        "colab": {}
      },
      "source": [
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "y_val = np.array(y_train[:10000])\n",
        "partial_y_train = np.array(y_train[10000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E8Kpo5G3OJEY"
      },
      "source": [
        "Then we start to train the model for 40 epochs in mini-batches of 512 samples and monitor the model's loss and accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "99_z39KAxjPi",
        "outputId": "46668cd8-d3f7-4952-f6fd-75cca1d26e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "15000/15000 [==============================] - 5s 310us/step - loss: 0.6924 - acc: 0.5643 - val_loss: 0.6913 - val_acc: 0.6404\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 3s 208us/step - loss: 0.6900 - acc: 0.6511 - val_loss: 0.6889 - val_acc: 0.6565\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 3s 208us/step - loss: 0.6873 - acc: 0.6581 - val_loss: 0.6863 - val_acc: 0.6616\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 3s 209us/step - loss: 0.6841 - acc: 0.6752 - val_loss: 0.6828 - val_acc: 0.6701\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 3s 210us/step - loss: 0.6805 - acc: 0.6821 - val_loss: 0.6792 - val_acc: 0.6733\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 3s 210us/step - loss: 0.6765 - acc: 0.6843 - val_loss: 0.6752 - val_acc: 0.6744\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 3s 210us/step - loss: 0.6719 - acc: 0.6859 - val_loss: 0.6708 - val_acc: 0.6767\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 3s 211us/step - loss: 0.6672 - acc: 0.6857 - val_loss: 0.6663 - val_acc: 0.6836\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 3s 212us/step - loss: 0.6622 - acc: 0.6904 - val_loss: 0.6613 - val_acc: 0.6820\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 3s 212us/step - loss: 0.6569 - acc: 0.6899 - val_loss: 0.6563 - val_acc: 0.6840\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 3s 212us/step - loss: 0.6517 - acc: 0.6913 - val_loss: 0.6512 - val_acc: 0.6884\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 3s 213us/step - loss: 0.6461 - acc: 0.6935 - val_loss: 0.6460 - val_acc: 0.6842\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 3s 213us/step - loss: 0.6406 - acc: 0.6958 - val_loss: 0.6413 - val_acc: 0.6961\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 3s 214us/step - loss: 0.6352 - acc: 0.7003 - val_loss: 0.6359 - val_acc: 0.6955\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 3s 214us/step - loss: 0.6298 - acc: 0.7044 - val_loss: 0.6308 - val_acc: 0.6991\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 3s 216us/step - loss: 0.6242 - acc: 0.7034 - val_loss: 0.6264 - val_acc: 0.7033\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 3s 216us/step - loss: 0.6190 - acc: 0.7079 - val_loss: 0.6207 - val_acc: 0.6998\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 3s 215us/step - loss: 0.6139 - acc: 0.7073 - val_loss: 0.6161 - val_acc: 0.6987\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 3s 215us/step - loss: 0.6087 - acc: 0.7133 - val_loss: 0.6117 - val_acc: 0.7105\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 3s 217us/step - loss: 0.6036 - acc: 0.7161 - val_loss: 0.6070 - val_acc: 0.7140\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 3s 215us/step - loss: 0.5989 - acc: 0.7183 - val_loss: 0.6025 - val_acc: 0.7153\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 3s 216us/step - loss: 0.5945 - acc: 0.7185 - val_loss: 0.5977 - val_acc: 0.7166\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 3s 217us/step - loss: 0.5896 - acc: 0.7236 - val_loss: 0.5933 - val_acc: 0.7153\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 3s 218us/step - loss: 0.5856 - acc: 0.7231 - val_loss: 0.5893 - val_acc: 0.7185\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 3s 217us/step - loss: 0.5814 - acc: 0.7265 - val_loss: 0.5855 - val_acc: 0.7232\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 3s 218us/step - loss: 0.5772 - acc: 0.7290 - val_loss: 0.5818 - val_acc: 0.7243\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 3s 218us/step - loss: 0.5732 - acc: 0.7304 - val_loss: 0.5781 - val_acc: 0.7259\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 3s 218us/step - loss: 0.5696 - acc: 0.7302 - val_loss: 0.5746 - val_acc: 0.7266\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 3s 217us/step - loss: 0.5660 - acc: 0.7325 - val_loss: 0.5718 - val_acc: 0.7291\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 3s 219us/step - loss: 0.5628 - acc: 0.7339 - val_loss: 0.5683 - val_acc: 0.7289\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 3s 217us/step - loss: 0.5598 - acc: 0.7363 - val_loss: 0.5656 - val_acc: 0.7318\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 3s 219us/step - loss: 0.5564 - acc: 0.7379 - val_loss: 0.5622 - val_acc: 0.7318\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 3s 219us/step - loss: 0.5534 - acc: 0.7385 - val_loss: 0.5601 - val_acc: 0.7334\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 3s 219us/step - loss: 0.5507 - acc: 0.7406 - val_loss: 0.5569 - val_acc: 0.7346\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 3s 220us/step - loss: 0.5481 - acc: 0.7417 - val_loss: 0.5546 - val_acc: 0.7354\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 3s 218us/step - loss: 0.5455 - acc: 0.7437 - val_loss: 0.5522 - val_acc: 0.7375\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 3s 219us/step - loss: 0.5432 - acc: 0.7431 - val_loss: 0.5500 - val_acc: 0.7392\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 3s 220us/step - loss: 0.5410 - acc: 0.7450 - val_loss: 0.5480 - val_acc: 0.7367\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 3s 221us/step - loss: 0.5392 - acc: 0.7449 - val_loss: 0.5459 - val_acc: 0.7407\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 3s 220us/step - loss: 0.5370 - acc: 0.7481 - val_loss: 0.5451 - val_acc: 0.7357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i_9a_rybhG5J"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EYLH8kOgOo9W"
      },
      "source": [
        "To evaulate the model on test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFMt2Q7b3taP",
        "outputId": "8c283171-9f50-4fa9-bd35-c8cba585f9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 4s 148us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9RrKiPHcAmQU",
        "outputId": "6a0d7e3a-988b-425d-867d-cd13029bb490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(results)\n",
        "# loss, accuracay "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.545550052242279, 0.73616]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pW7IpHxMO6qp"
      },
      "source": [
        "Our first model accuracy using one-hot vectors is \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwZk_yoWhPJB"
      },
      "source": [
        "### Plotting the accuracy graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIDPH1J7PMzN"
      },
      "source": [
        "To plot a graph of accuracy and loss over time we can use Matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LS9k2vvSAqB7",
        "outputId": "5a0b6dc8-cdba-4140-b1bd-388677bfbd1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QCewvf_1yD0",
        "colab_type": "code",
        "outputId": "048a27ca-9528-4547-d82c-f925a6e95e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "VOCAB_SIZE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a7OwOQw4h8RX"
      },
      "source": [
        "### Neural Network model using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l-QzOMO_P4jc"
      },
      "source": [
        "Now instead of one-hot vectors, we want to use embedding. We change our first layer in model1 to an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFrCsL-NBFVL",
        "outputId": "a3424fb3-9f9c-4064-d309-b84327f9e6cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "VOCAB_SIZE= 10000\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(VOCAB_SIZE,16,input_length=MAX_SEQUENCE_LENGTH))\n",
        "model2.add(GlobalAveragePooling1DMasked())\n",
        "model2.add(Dense(16,activation='relu'))\n",
        "model2.add(Dense(1,activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# put the code here\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history2 = model2.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model2.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 256, 10000)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.6917 - acc: 0.5659 - val_loss: 0.6894 - val_acc: 0.7203\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.6852 - acc: 0.7257 - val_loss: 0.6805 - val_acc: 0.7408\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.6713 - acc: 0.7594 - val_loss: 0.6632 - val_acc: 0.7582\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.6470 - acc: 0.7766 - val_loss: 0.6359 - val_acc: 0.7520\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.6123 - acc: 0.7879 - val_loss: 0.5994 - val_acc: 0.7836\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.5685 - acc: 0.8088 - val_loss: 0.5579 - val_acc: 0.8026\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.5213 - acc: 0.8293 - val_loss: 0.5146 - val_acc: 0.8176\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.4744 - acc: 0.8471 - val_loss: 0.4744 - val_acc: 0.8325\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.4317 - acc: 0.8622 - val_loss: 0.4382 - val_acc: 0.8425\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.3942 - acc: 0.8715 - val_loss: 0.4089 - val_acc: 0.8517\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.3626 - acc: 0.8806 - val_loss: 0.3858 - val_acc: 0.8572\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.3361 - acc: 0.8878 - val_loss: 0.3658 - val_acc: 0.8632\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.3136 - acc: 0.8949 - val_loss: 0.3501 - val_acc: 0.8680\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.2942 - acc: 0.9003 - val_loss: 0.3376 - val_acc: 0.8713\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.2776 - acc: 0.9059 - val_loss: 0.3272 - val_acc: 0.8745\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.2622 - acc: 0.9109 - val_loss: 0.3187 - val_acc: 0.8757\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.2488 - acc: 0.9140 - val_loss: 0.3115 - val_acc: 0.8780\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.2366 - acc: 0.9183 - val_loss: 0.3060 - val_acc: 0.8789\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.2250 - acc: 0.9230 - val_loss: 0.3007 - val_acc: 0.8821\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.2146 - acc: 0.9272 - val_loss: 0.2967 - val_acc: 0.8819\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.2053 - acc: 0.9295 - val_loss: 0.2936 - val_acc: 0.8827\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.1962 - acc: 0.9336 - val_loss: 0.2908 - val_acc: 0.8840\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.1884 - acc: 0.9366 - val_loss: 0.2891 - val_acc: 0.8845\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 15us/step - loss: 0.1799 - acc: 0.9415 - val_loss: 0.2875 - val_acc: 0.8850\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.1726 - acc: 0.9444 - val_loss: 0.2873 - val_acc: 0.8838\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.1659 - acc: 0.9475 - val_loss: 0.2861 - val_acc: 0.8847\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.1589 - acc: 0.9509 - val_loss: 0.2856 - val_acc: 0.8858\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.1528 - acc: 0.9526 - val_loss: 0.2859 - val_acc: 0.8847\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 13us/step - loss: 0.1472 - acc: 0.9546 - val_loss: 0.2862 - val_acc: 0.8853\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.1413 - acc: 0.9583 - val_loss: 0.2870 - val_acc: 0.8861\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.1357 - acc: 0.9603 - val_loss: 0.2883 - val_acc: 0.8855\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.1305 - acc: 0.9617 - val_loss: 0.2894 - val_acc: 0.8859\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.1257 - acc: 0.9639 - val_loss: 0.2909 - val_acc: 0.8865\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.1209 - acc: 0.9662 - val_loss: 0.2936 - val_acc: 0.8847\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 15us/step - loss: 0.1165 - acc: 0.9679 - val_loss: 0.2947 - val_acc: 0.8859\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.1122 - acc: 0.9695 - val_loss: 0.2969 - val_acc: 0.8841\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.1080 - acc: 0.9703 - val_loss: 0.3001 - val_acc: 0.8844\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.1039 - acc: 0.9718 - val_loss: 0.3019 - val_acc: 0.8835\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 14us/step - loss: 0.1001 - acc: 0.9730 - val_loss: 0.3051 - val_acc: 0.8838\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.0967 - acc: 0.9745 - val_loss: 0.3077 - val_acc: 0.8833\n",
            "25000/25000 [==============================] - 1s 40us/step\n",
            "[0.32691408640861513, 0.87344]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I4zIPJDcTPq3",
        "outputId": "2c011ca5-e233-4ece-f941-295521df2f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model2.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 35us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "waS96edDTRyL",
        "outputId": "43e14271-bc3c-44c0-bab6-1a2180fa507e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.32691408640861513, 0.87344]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XB7aveVzTC5a",
        "outputId": "09feb44e-26ae-4434-cdc0-7a190541e995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history2.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5xVZdn/8c+XkTPIOTVAQMVwOMOE\nFuYpUTykhpog/R7JjPQRM7XHR8OfGmWW6aNWdMB+pilKZI9GZpEHzCwrBjkkeEIOOogyDKCc5Hj9\n/rjXnlmz2Xtmz2HP3rPner9e6zXruPe114Z17fu+17pvmRnOOedcsla5DsA551x+8gThnHMuJU8Q\nzjnnUvIE4ZxzLiVPEM4551LyBOGccy4lTxAuY5KKJG2TdHhj7ptLko6S1Oj3eks6VdKa2PLrkj6T\nyb71eK9fSPpmfY93Lp2Dch2Ayx5J22KLHYBdwL5o+atmNrsur2dm+4BOjb1vS2Bmn2iM15F0GfBF\nMzsp9tqXNcZrO5fME0QBM7PKC3T0C/UyM3sm3f6SDjKzvU0Rm3O18X+PuedVTC2YpO9I+rWkRyVt\nBb4o6VOS/iFpi6T1kn4oqXW0/0GSTFL/aPnhaPsfJW2V9JKkAXXdN9p+hqQ3JH0g6UeS/iZpSpq4\nM4nxq5JWStos6YexY4sk3S2pQtIqYHwN52e6pDlJ62ZK+p9o/jJJr0af563o13261yqTdFI030HS\nQ1Fsy4HRSfveJGlV9LrLJZ0TrR8K/Bj4TFR9tzF2bm+NHX959NkrJD0h6bBMzk1dznMiHknPSNok\n6T1J18fe5/9G5+RDSaWSPp6qOk/Si4nvOTqfL0Tvswm4SdJASQui99gYnbcuseP7RZ+xPNp+r6R2\nUczHxPY7TNIOST3SfV6Xgpn51AImYA1watK67wC7gc8Rfiy0Bz4JHEsoXR4BvAFMi/Y/CDCgf7T8\nMLARKAFaA78GHq7Hvh8DtgLnRtuuBfYAU9J8lkxi/B3QBegPbEp8dmAasBzoA/QAXgj/DVK+zxHA\nNqBj7LU3ACXR8ueifQScAuwEhkXbTgXWxF6rDDgpmr8TeB7oBvQDViTt+wXgsOg7uTiK4ZBo22XA\n80lxPgzcGs2fFsU4AmgH/AR4LpNzU8fz3AV4H7gaaAscDIyJtt0ILAUGRp9hBNAdOCr5XAMvJr7n\n6LPtBa4Aigj/Ho8GPgu0if6d/A24M/Z5XonOZ8do/7HRtlnAbbH3uQ54PNf/D5vblPMAfGqiLzp9\ngniuluO+Afwmmk910f9ZbN9zgFfqse+lwF9j2wSsJ02CyDDG42Lb/xf4RjT/AqGqLbHtzOSLVtJr\n/wO4OJo/A3i9hn2fBK6M5mtKEG/HvwvgP+P7pnjdV4CzovnaEsSDwHdj2w4mtDv1qe3c1PE8/x9g\nYZr93krEm7Q+kwSxqpYYLki8L/AZ4D2gKMV+Y4HVgKLlJcCExv5/VeiTVzG5d+ILkgZJ+kNUZfAh\nMAPoWcPx78Xmd1Bzw3S6fT8ej8PC/+iydC+SYYwZvRewtoZ4AR4BJkXzF0fLiTjOlvTPqPpjC+HX\ne03nKuGwmmKQNEXS0qiaZAswKMPXhfD5Kl/PzD4ENgO9Y/tk9J3Vcp77EhJBKjVtq03yv8dDJc2V\ntC6K4YGkGNZYuCGiGjP7G6E0crykIcDhwB/qGVOL5QnCJd/i+XPCL9ajzOxg4GbCL/psWk/4hQuA\nJFH9gpasITGuJ1xYEmq7DXcucKqk3oQqsEeiGNsDjwG3E6p/ugJ/zjCO99LFIOkI4KeEapYe0eu+\nFnvd2m7JfZdQbZV4vc6Eqqx1GcSVrKbz/A5wZJrj0m3bHsXUIbbu0KR9kj/f9wl33w2NYpiSFEM/\nSUVp4vgV8EVCaWeume1Ks59LwxOES9YZ+ADYHjXyfbUJ3vNJYJSkz0k6iFCv3StLMc4Fvi6pd9Rg\n+d817Wxm7xGqQR4gVC+9GW1qS6gXLwf2STqbUFeeaQzflNRV4TmRabFtnQgXyXJCrvwKoQSR8D7Q\nJ95YnORR4MuShklqS0hgfzWztCWyGtR0nucBh0uaJqmtpIMljYm2/QL4jqQjFYyQ1J2QGN8j3AxR\nJGkqsWRWQwzbgQ8k9SVUcyW8BFQA31Vo+G8vaWxs+0OEKqmLCcnC1ZEnCJfsOuASQqPxzwmNyVll\nZu8DFwH/Q/gPfySwmPDLsbFj/CnwLPBvYCGhFFCbRwhtCpXVS2a2BbgGeJzQ0HsBIdFl4hZCSWYN\n8EdiFy8zWwb8CPhXtM8ngH/Gjn0aeBN4X1K8qihx/J8IVUGPR8cfDkzOMK5kac+zmX0AjAPOJySt\nN4ATo80/AJ4gnOcPCQ3G7aKqw68A3yTcsHBU0mdL5RZgDCFRzQN+G4thL3A2cAyhNPE24XtIbF9D\n+J53mdnf6/jZHVUNOM7ljajK4F3gAjP7a67jcc2XpF8RGr5vzXUszZE/KOfygqTxhDuGdhJuk9xD\n+BXtXL1E7TnnAkNzHUtz5VVMLl8cD6wi1L2fDnzeGxVdfUm6nfAsxnfN7O1cx9NceRWTc865lLwE\n4ZxzLqWCaYPo2bOn9e/fP9dhOOdcs7Jo0aKNZpbytvKCSRD9+/entLQ012E451yzIiltbwJexeSc\ncy4lTxDOOedS8gThnHMupYJpg0hlz549lJWV8dFHH+U6FFeDdu3a0adPH1q3Tte9kHMuFwo6QZSV\nldG5c2f69+9P6CDU5Rszo6KigrKyMgYMGFD7Ac65JlPQVUwfffQRPXr08OSQxyTRo0cPL+U5Vw+z\nZ0P//tCqVfg7e3bjvn5BJwjAk0Mz4N+Rc6nVlABmz4apU2HtWjALf6dObdwkUfAJwjnncqm2i3x9\nE8D06bBjR/X32rEjrG8sniCyqKKighEjRjBixAgOPfRQevfuXbm8e/fujF7jS1/6Eq+//nqN+8yc\nOZPZjV22dM4BtVfj1Pci39AE8HaaLgjTra+XXA+K3VjT6NGjLdmKFSsOWFeThx8269fPTAp/H364\nTofX6JZbbrEf/OAHB6zfv3+/7du3r/HeqJmq63flXGOp6f/9ww+bdehgFi7hYerQoWqf2rb361d9\nW2Lq16/mbWYhnlTbpdpfuy6AUktzXfUSRKQp6vMSVq5cSXFxMZMnT2bw4MGsX7+eqVOnUlJSwuDB\ng5kxY0blvscffzxLlixh7969dO3alRtuuIHhw4fzqU99ig0bNgBw0003cc8991Tuf8MNNzBmzBg+\n8YlP8Pe/h4G0tm/fzvnnn09xcTEXXHABJSUlLFmy5IDYbrnlFj75yU8yZMgQLr/8cizq7feNN97g\nlFNOYfjw4YwaNYo1a9YA8N3vfpehQ4cyfPhwpjdm2da5RpLNapyG/MqvrQRweJrR0hPrb7sNOnSo\nvq1Dh7C+0aTLHM1tamgJorGycTrxEsSbb75pkmzhwoWV2ysqKszMbM+ePXb88cfb8uXLzcxs7Nix\ntnjxYtuzZ48B9tRTT5mZ2TXXXGO33367mZlNnz7d7r777sr9r7/+ejMz+93vfmenn366mZndfvvt\n9p//+Z9mZrZkyRJr1aqVLV68+IA4E3Hs37/fJk6cWPl+o0aNsnnz5pmZ2c6dO2379u02b948O/74\n423Hjh3Vjq0PL0G4hkhXCmjIL3yz2n/FN+RXfm3vXVvsNX3uusBLELVrkvq8mCOPPJKSkpLK5Ucf\nfZRRo0YxatQoXn31VVasWHHAMe3bt+eMM84AYPTo0ZW/4pNNmDDhgH1efPFFJk6cCMDw4cMZPHhw\nymOfffZZxowZw/Dhw/nLX/7C8uXL2bx5Mxs3buRzn/scEB5s69ChA8888wyXXnop7du3B6B79+51\nPxHOZaC+pYCG1uPX9iu+Ib/yaysBTJ4Ms2ZBv34ghb+zZoX1CZMnw5o1sH9/+Du5vqOPp+EJIlLb\nF93YOnbsWDn/5ptvcu+99/Lcc8+xbNkyxo8fn/K5gDZt2lTOFxUVsXfv3pSv3bZt21r3SWXHjh1M\nmzaNxx9/nGXLlnHppZf68wmuSWSrGijb1TgNucjnQwKojSeISJPU56Xx4Ycf0rlzZw4++GDWr1/P\n/PnzG/09xo4dy9y5cwH497//nbKEsnPnTlq1akXPnj3ZunUrv/3tbwHo1q0bvXr14ve//z0QHkDc\nsWMH48aN4/7772fnzp0AbNq0qdHjdoUvm3fzNDQB1HYRb+hFPtcJoDaeICKZfNHZMmrUKIqLixk0\naBD/8R//wdixYxv9Pa666irWrVtHcXEx3/rWtyguLqZLly7V9unRoweXXHIJxcXFnHHGGRx77LGV\n22bPns1dd93FsGHDOP744ykvL+fss89m/PjxlJSUMGLECO6+++5Gj9sVhppKCNmsBmqKapx8v8g3\nSLrGieY2NcZtroVsz549tnPnTjMze+ONN6x///62Z8+eHEdVxb+r5q0ht4o29HbOTG5Fzdbt64WA\nGhqps3rRBsYDrwMrgRtSbO8HPAssA54H+sS27QOWRNO82t7LE0TNNm/ebKNGjbJhw4bZ0KFDbf78\n+bkOqRr/rvJftu4Uype7eVqqnCQIoAh4CzgCaAMsBYqT9vkNcEk0fwrwUGzbtrq8nyeI5s2/q9yr\nbymgobeKegLIrVwliE8B82PLNwI3Ju2zHOgbzQv4MLbNE0QL4t9VbjWkFNAYT/x6AsidmhJENhup\newPvxJbLonVxS4EJ0fzngc6SekTL7SSVSvqHpPNSvYGkqdE+peXl5Y0Zu3MFJ1sNxY3xxG9BN/Q2\nY7m+i+kbwImSFgMnAusIbQ8A/cysBLgYuEfSkckHm9ksMysxs5JevXo1WdDO5aOGPEuQ6zuFXJ5K\nV7Ro6EQGVUxJ+3cCytJsewC4oKb38yqm5s2/q4bJdUOxVxE1X+SoimkhMFDSAEltgInAvPgOknpK\nSsRwI3B/tL6bpLaJfYCxwIFPduW5k08++YCH3u655x6uuOKKGo/r1KkTAO+++y4XXHBByn1OOukk\nSktLa3yde+65hx2xeoMzzzyTLVu2ZBK6a2Ya+ixBYzww5lVEhSdrCcLM9gLTgPnAq8BcM1suaYak\nc6LdTgJel/QGcAiQqJU8BiiVtBRYAHzPzJpdgpg0aRJz5syptm7OnDlMmjQpo+M//vGP89hjj9X7\n/ZMTxFNPPUXXrl3r/Xout2qqQmpolxLNodsHlwPpihbNbcrHKqaKigrr1auX7dq1y8zMVq9ebX37\n9rX9+/fb1q1b7ZRTTrGRI0fakCFD7Iknnqg8rmPHjpX7Dx482MzMduzYYRdddJENGjTIzjvvPBsz\nZkxlb7CXX365jR492oqLi+3mm282M7N7773XWrdubUOGDLGTTjrJzMz69etn5eXlZmZ211132eDB\ng23w4MGVPcGuXr3aBg0aZJdddpkVFxfbuHHjKntqjZs3b56NGTPGRowYYZ/97GftvffeMzOzrVu3\n2pQpU2zIkCE2dOhQe+yxx8zM7I9//KONHDnShg0bZqecckrKc5Xr7ypfZOtZg0xuJXUtE7l6UK4p\np9oSxNVXm514YuNOV19d26k3O+ussyov/rfffrtdd911ZhaebP7ggw/MzKy8vNyOPPJI279/v5ml\nThB33XWXfelLXzIzs6VLl1pRUVFlgkh0s71371478cQTbenSpWZWPSHEl0tLS23IkCG2bds227p1\nqxUXF9vLL79sq1evtqKiospuwC+88EJ76KGHDvhMmzZtqoz1vvvus2uvvdbMzK6//nq7OnZSNm3a\nZBs2bLA+ffrYqlWrqsWazBNEw5418GcJXH3VlCByfRdTwYtXM8Wrl8yMb37zmwwbNoxTTz2VdevW\n8f7776d9nRdeeIEvfvGLAAwbNoxhw4ZVbps7dy6jRo1i5MiRLF++PGVHfHEvvvgin//85+nYsSOd\nOnViwoQJ/PWvfwVgwIABjBgxAkjfpXhZWRmnn346Q4cO5Qc/+AHLly8H4JlnnuHKK6+s3K9bt278\n4x//4IQTTmDAgAGAdwle31tNa6tC8ioilw0H5TqAphINuNbkzj33XK655hpefvllduzYwejRo4HQ\n+V15eTmLFi2idevW9O/fv15da69evZo777yThQsX0q1bN6ZMmdKgLroTXYVD6C480VNr3FVXXcW1\n117LOeecw/PPP8+tt95a7/crNIkxCBLPB9x2W9WFOHGraSIJJG41hbBPbc8arF174LZ420KiC2nn\nGouXILKsU6dOnHzyyVx66aXVGqc/+OADPvaxj9G6dWsWLFjA2lT/+2NOOOEEHnnkEQBeeeUVli1b\nBoSuwjt27EiXLl14//33+eMf/1h5TOfOndm6desBr/WZz3yGJ554gh07drB9+3Yef/xxPvOZz2T8\nmT744AN69w7PPD744IOV68eNG8fMmTMrlzdv3sxxxx3HCy+8wOrVq4HC7hK8od1WN+RZA+eywRNE\nE5g0aRJLly6tliAmT55MaWkpQ4cO5Ve/+hWDBg2q8TWuuOIKtm3bxjHHHMPNN99cWRIZPnw4I0eO\nZNCgQVx88cXVugqfOnUq48eP5+STT672WqNGjWLKlCmMGTOGY489lssuu4yRI0dm/HluvfVWLrzw\nQkaPHk3Pnj0r1990001s3ryZIUOGMHz4cBYsWECvXr2YNWsWEyZMYPjw4Vx00UUZv08+yma31TUl\nAX/YzOVEusaJ5jbl411MLnPN4bvKdrfViffwhmTXlPBGaucy05ASgvdJ5AqNJwjnIg3tr8j7JHKF\npuATRChBuXyWL99RQ0sIfqupKzQFnSDatWtHRUVF3lyA3IHMjIqKCtq1a9dk75muGqmhJQTwBOAK\nS0E/B9GnTx/KysrwsSLyW7t27ejTp0+jvV59n0Wo7VmDxGuke23nCo0K5dd1SUmJ1da7qSt8yQkA\nwq/8RFVP//6pk0C/fuFiX9OxzhUiSYssjL1zgIKuYnKFKVvPIngjsnPVeQnCNSu1lRBatQp3ICWT\nQrtATSWIFN1OOVfwvAThCkZTPIvgnAuymiAkjZf0uqSVkm5Isb2fpGclLZP0vKQ+sW2XSHozmi7J\nZpwuvzRkYBx/FsG5RpTuEeuGTkAR8BZwBNAGWAoUJ+3zG+CSaP4U4KFovjuwKvrbLZrvVtP7pepq\nwzU/DR0YJ/Ea3l2Fc5khR11tjAFWmtkqM9sNzAHOTdqnGHguml8Q23468LSZbTKzzcDTwPgsxury\nRG1VSP4sgnNNJ5sJojfwTmy5LFoXtxSYEM1/HugsqUeGxyJpqqRSSaX+rEPz0ZAqJK8icq7p5LqR\n+hvAiZIWAycC64B9mR5sZrPMrMTMSnr16pWtGF0jqq2/o9oamcFLCM41lWwmiHVA39hyn2hdJTN7\n18wmmNlIYHq0bksmx7rmqTGqkJxzTSObCWIhMFDSAEltgInAvPgOknpKSsRwI3B/ND8fOE1SN0nd\ngNOida6ZqG9/R16F5Fz+yFpfTGa2V9I0woW9CLjfzJZLmkFoNZ8HnATcLsmAF4Aro2M3Sfo2IckA\nzDCzwh2rssA0pL8j8LGVncsX/iS1a3Te35FzzUdNT1IXdG+uLjdq6+8IvEfUhtq7Fz76CHbtCn/j\nkwTdu0PPntCxY1jOxL59sHUrbNkCmzbB5s1VfxPzH34YXrN7d+jWLUyJ+e7doVOnENOOHbBzZ/W/\niWnrVti2LfxNnjcLPxjat6/6G583g927w3vs3l19fteu8F7x943P790Lhx0GfftCnz7hb3z+4IPD\nZ9y4ESoqDpwOOij1cZ06Zfe7ziVPEK5eaupSO5Nus1tiQjCD7dvDhXDTJigvDxej8vLq8xUVVRe3\n5It/YtqX4b1+bdpAjx7Vp1atwoX+gw/C38T8tm21v9bBB4fPsHNnw89HmzbQuXO4wHbuHCYpnJvk\nxLJzZ/U+ttq0qZratq2ajyeXrl2rL7dqBe++C2VlsHgxvP9+ZnG2bx/O2549qY/p2jUki7Ztwz57\n94a/iWnv3jB17BjOX2Lq0qVqvmPH8J2mO94M2rUL79Gu3YFT795wbvJTZo3AE4Srs5raGCZPTl+N\n1FzvRDKDdetg0aKq6d13wwWnqKhqOuigqvndu6v/Ot62LUw11eh27Qq9eoWLUceO4QISvwgkLg5t\n24aLVvJFIrF9//7Uv4ArKuC110IMXbqEqW/f6heqgw9OXTLo1i28Z6I08tFHVSWLeElj69YDf/XH\n5zt0qEoGbdrU7TvYtSuc89atMy8V1WT37vC9lpXBO++ERJmcTHv0CHEn7NoVvvt33glT4tiysnAx\nb926ajrooKr5oqLw/yGRlCsqYPXqquS8Y0fVZ0t1fOK94z8S4o47LjsJwtsgXJ1l0iNqTSWMfLN/\nf7h4x39Vr18PL78cksHLL8OGDWHfVq3gmGPgiCPCcfv2HTjt3Zv613FivlOncOHp2TMkhERSSFwI\nXMtjVrekl6hqiyeLQw6p33vX1AbhCcLVWW1daueTffvCL7yVK+HNN6v+rl4d6to/+CD86k2lqAgG\nD4ZRo2D06DANGxZ+3TtXKLyR2tVZQ9oYmsKmTeHCH68+iTcubtwYksCqVeGXVkK7dnDUUaEE0KPH\ngXXBieWePUNyiFcvONfSeIJwB8i3NoaNG6vq/hPVPukG90lU3/ToEaqCzjknJISBA8Pfj388lICc\nc7XzKiZ3gFy2MVRUQGlp1fTyy9Vvmz3yyFDVM2pUuOAnkkHPnqExtW3bhsfgXEvibRCuTpqqjeHD\nD0MCWLgwJIOFC0O1UMJRR0FJSVUbwKhR4U4f51zj8TYIVyfZamMwg6VL4cknw/Svf1Ulov79QzL4\n6lfhk5/0ZOBcPvAE0ULVVEXUmG0MO3fCc89VJYWysrB+zBi4+eZw//bo0eFWT+dcfvEE0QLV1gjd\n0O4wdu6E3/0OHnkEnnkmLHfqBKedBjNmwJln1v+ebedc0/E2iBYok0boujKDv/8dHnwQ5s4Nzxf0\n7QvnnQdnnw0nnugNyM7lI2+DcNXUNiZDXaxdCw89FBLDypWhKuqCC+CSS+Ckk/yWUueaM08QLVBD\nG6E3boT//V949FF4/vmw7qSTQpXU+eeH7iScc81fVn/fSRov6XVJKyXdkGL74ZIWSFosaZmkM6P1\n/SXtlLQkmn6WzThbmvoM67llCzzwAJxxBhx6aLjbaN260KawejUsWABTpnhycK6QZK0EIakImAmM\nA8qAhZLmmdmK2G43AXPN7KeSioGngP7RtrfMbES24mvJMm2E3rYN5s2DX/8a/vSn0GVF//7wjW/A\nxIkwfHjj9KrpnMtP2SxBjAFWmtkqM9sNzAGSO6Q14OBovgvwbhbjaVHSjQmdMHlyaJDevz/8jSeH\nPXvgxz8OjdaTJ4euLaZNg3/+M/Rt9L3vwYgRnhycK3TZbIPoDbwTWy4Djk3a51bgz5KuAjoCp8a2\nDZC0GPgQuMnM/pr8BpKmAlMBDm/KnuLyXG23sdZk/ny45hp49VU4+WS49VY4/nhvbHauJcr1f/tJ\nwANm1gc4E3hIUitgPXC4mY0ErgUekXRw8sFmNsvMSsyspJc/aVVp+vTqD7lBWJ4+Pf0xr70GZ50F\n48eHqqQnnoBnn4UTTvDk4FxLlc3/+uuAvrHlPtG6uC8DcwHM7CWgHdDTzHaZWUW0fhHwFnB0FmMt\nKHW5jXXTJrj6ahgyBF58Ee68E5YvD6NTeRWScy1bNhPEQmCgpAGS2gATgXlJ+7wNfBZA0jGEBFEu\nqVfUyI2kI4CBwKosxlpQ0tW2xdfv2wc/+UnoEO/HP4avfCU8x3Dddf5Am3MuyFqCMLO9wDRgPvAq\n4W6l5ZJmSDon2u064CuSlgKPAlMsPNp9ArBM0hLgMeByM9uUrVgLTW23sS5eDJ/+NFx5JYwcCUuW\nwE9/6v0hOeeq8642ClSqzvjOOSd0kPfDH4bxE+6+GyZN8qok51qymrra8ObHAhW/jXX16lCCKC6G\ne+8NdzS99hpcfLEnB+dcep4gCtzataHkMGFCGHHt738P1UnduuU6MudcvvME0UzV9iBcYp/i4jAe\nw513hgfejjuuqSN1zjVX3llfM5TJg3D33gtf/3roZvtXv2r4aHDOuZbHSxDNUE0PwpmFhuivfz2M\nxfCnP3lycM7Vj5cgmqF0D8KtXRv6TPrJT+DSS+HnP4eD/Bt2ztWTlyCaoXQlgg4dQnL4r/+CX/zC\nk4NzrmE8QTRDqR6Ea9UqVDN9//twxx1++6pzruE8QTRDkyfDrFmhO24IXWOYwX33wfXX5zY251zh\n8ATRTE2eDC+9BEOHhuTwm9/AZZflOirnXCHxWupmatUqGDcO3n8f/vAHOPXU2o9xzrm68BJEHkv3\nMNwrr4RBfLZsCWM2eHJwzmWDlyDyVLqH4VauDA/BtW8PL7wAgwfnNk7nXOGqtQQh6SpJ3nNPE0v3\nMNy3vhX6VHrxRU8OzrnsyqSK6RBgoaS5ksZLfgNlU0j3MJxZSA4DBjRtPM65lqfWBGFmNxFGdPt/\nwBTgTUnflXRklmNr0dI9DNenDxx6aNPG4pxrmTJqpI5GeXsvmvYC3YDHJN1R03FRieN1SSsl3ZBi\n++GSFkhaLGmZpDNj226Mjntd0ul1+lQFINXDcO3bw/e+l5t4nHMtTyZtEFdLWgTcAfwNGGpmVwCj\ngfNrOK4ImAmcARQDkyQVJ+12E2Eo0pGEMat/Eh1bHC0PBsYDP0mMUd1STJ5c1TMrhBLFffdVX+ec\nc9mUyV1M3YEJZrY2vtLM9ks6u4bjxgArzWwVgKQ5wLnAivjLAAdH812Ad6P5c4E5ZrYLWC1pZfR6\nL2UQb0FYsAB++Us47TR48klo3TrXETnnWppMqpj+CGxKLEg6WNKxAGb2ag3H9QbeiS2XRevibgW+\nKKkMeAq4qg7HImmqpFJJpeXl5Rl8lObhjTfg/PPh6KNh7lxPDs653MgkQfwU2BZb3hatawyTgAfM\nrA9wJvCQpIwf3jOzWWZWYmYlvXr1aqSQcquiAs46K/TE+uST0KVLriNyzrVUmVyMFTVSA6Fqicyq\nptYBfWPLfaJ1cV8G5kav+xLQDuiZ4bHNXvKT0g88EMaOfucdeOIJv5XVOZdbmSSIVZK+Jql1NF0N\nrMrguIXAQEkDJLUhNDrPS4gK37AAABQjSURBVNrnbeCzAJKOISSI8mi/iZLaShpAuM32X5l9pOYh\n8aT02rXh2Ya1a0Nney+8ENoePv3pXEfonGvpMkkQlwOfJvyCLwOOBabWdpCZ7QWmAfOBVwl3Ky2X\nNEPSOdFu1wFfkbQUeBSYYsFyQsliBfAn4Eoz21e3j5bfUj0pvW9fqFKaNCk3MTnnXJxitUfNWklJ\niZWWluY6jIy1ahVKDqkUyFfinGsGJC0ys5JU22ptS5DUjtBWMJhQBQSAmV3aaBG2QIcfHqqVkiUG\nAXLOuVzLpIrpIeBQ4HTgL4QG463ZDKoluO228GR0XIcOYb1zzuWDTBLEUWb2f4HtZvYgcBahHcLV\nIt14DhCeiL7wwqrlfv3CMKL+pLRzLl9kcrvqnujvFklDCP0xfSx7IRWGdOM5QEgCO3fC00/DCSfA\n88+D95HrnMs3mZQgZkXjQdxEuP10BfD9rEZVANKN5zB9epj/2c9g/XqYMcOTg3MuP9VYgoieav7Q\nzDYDLwBHNElUBSDdeA5vvw3bt4deWT/7WTjxxKaNyznnMlVjCSJ6avr6JoqloKQbz+Hww2HmTNiw\nAb797aaNyTnn6iKTKqZnJH1DUl9J3RNT1iNr5lKN59ChA9x0E9xxB5xxBnzqU7mJzTnnMpFJI/VF\n0d8rY+sMr26qUeJupOnTQ7XS4YeHpLF6deiQ71vfym18zjlXm1oThJl5l3H1lDzoz5YtoQO+c86B\nT34yd3E551wmMhlR7j9STU0RXL6r6TmHVO6+OyQJLz0455qDTKqY4r912xF6X30Z+FVWImomanvO\nIVlFRUgQ558PI0Y0XZzOOVdfmVQxXRVfltQVmJO1iJqJmp5zSJUg7rwTtm2DW29tkvCcc67BMh69\nLWY70OLbJWp6ziHZhg3wox/BRRfBkCHZjcs55xpLJr25/p5w1xKEhFJMNApcS5auN9Z27eAPf4DT\nTw/DhkK4rXXnTi89OOeal0zaIO6Mze8F1ppZWSYvLmk8cC9QBPzCzL6XtP1u4ORosQPwMTPrGm3b\nB/w72va2mZ1DHpk4Eb6f1OHIQQdB69Zw9tlw2GFwySVhfOmZM+GLX4RPfCI3sTrnXH3UOmBQNOTn\nejP7KFpuDxxiZmtqOa4IeAMYRxiJbiEwycxWpNn/KmBkYpwJSdvMrFOmH6QpBwx66SU49VTo2TMM\n7lNWVvWcwxe+EEoQ998PTz0VRokrKoLXXoOjjmqS8JxzLmMNGjAI+A1hyNGEfdG62u7kHwOsNLNV\nURBzgHMJnf2lMgm4JYN4cuqVV0Kp4LDD4MUX4dBDD9znvPPC9O678PDDIZF4cnDONTeZJIiDzGx3\nYsHMdktqk8FxvYF3YsuJ8awPIKkfoeH7udjqdpJKCdVa3zOzJ1IcN5VofOzD03V+1IhWrYLTTgsD\n/Tz9dOrkEPfxj8P13pOVc66ZyuQupnJJlfX/ks4FNjZyHBOBx8xsX2xdv6jYczFwj6Qjkw8ys1lm\nVmJmJb169WrkkKp7772QHHbtgj//OTwR7ZxzhSyTEsTlwGxJP46Wy4BMnqReB/SNLfeJ1qUykep9\nPWFm66K/qyQ9D4wE3srgfRvd5s3hrqT33oNnn4XBg3MRhXPONa1MHpR7CzhOUqdoeVuGr70QGBg1\ncq8jJIGLk3eSNAjoBrwUW9cN2GFmuyT1BMYCd2T4vo1qx45wV9Krr4bG52N9sFXnXAuRSV9M35XU\n1cy2mdk2Sd0kfae248xsLzANmA+8Csw1s+WSZsSrrAiJY45Vv53qGKBU0lJgAaENIl3jdtaYhe4z\nXnoJHnkExo1r6giccy53MrnNdbGZjUxa97KZjcpqZHWUjdtcf/Qj+NrXwsA+N93UqC/tnHN5oabb\nXDNppC6S1Db2Yu2BtjXsXxBefBGuvRY+9zn45jdzHY1zzjW9TBqpZwPPSvolIGAK8GA2g8q19evh\nwgvDnUoPPRS683bOuZYmk0bq70dtAacS+mSaD/TLdmC5snt3SA4ffhiedejSJdcROedcbmT62/h9\nQnK4EDiF0OhckK67Dv72t9BVxtKldRsQyDnnCknaEoSkowndX0wiPBj3a0Kj9snpjmnuHn4Yfvzj\n0Pawd2/dBgRyzrlCk/YuJkn7gb8CXzazldG6VWZ2RBPGl7GG3sW0ZAl8+tMwZgw880zoOylVd979\n+sGaNfWP0znn8kl972KaAKwHFki6T9JnCY3UBWfTJpgwAbp3h1//OnTbXZcBgZxzrhClTRBm9oSZ\nTQQGER5W+zrwMUk/lXRaUwWYbfv2hSqjsjJ47DE45JCwPl3ff03QJ6BzzuWFWhupzWy7mT1iZp8j\n9Ke0GPjvrEfWRN56C/71r/BQ3HHHVa2/7Tbo0KH6vh06hPXOOdcSZPIcRCUz2wzMiqaCcPTRYTCf\nnj2rr080RE+fHqqVEgMCeQO1c66lqFOCKFTpegqfPNkTgnOu5fJnhJ1zzqXkCcI551xKniCcc86l\n5AnCOedcSllNEJLGS3pd0kpJN6TYfrekJdH0hqQtsW2XSHozmi7JZpzOOecOlLW7mCQVATOBcYRx\nrBdKmhcfGc7MrontfxVh3GkkdQduAUoInQQuio7dnK14nXPOVZfNEsQYYKWZrTKz3cAc4Nwa9p8E\nPBrNnw48bWaboqTwNDA+i7E655xLks0E0Rt4J7ZcFq07gKR+wADgubocK2mqpFJJpeXl5Y0StHPO\nuSBfGqknAo+Z2b66HGRms8ysxMxKeqV72s0551y9ZDNBrAP6xpb7ROtSmUhV9VJdj3XOOZcF2UwQ\nC4GBkgZIakNIAvOSd5I0COgGvBRbPR84TVI3Sd2A06J1zjnnmkjW7mIys72SphEu7EXA/Wa2XNIM\noNTMEsliIjDHYiMXmdkmSd8mJBmAGWa2KVuxOuecO1DaEeWam4aOKOeccy1RfUeUc84514J5gnDO\nOZeSJwjnnHMpeYJwzjmXkicI55xzKXmCcM45l5InCOeccyl5gnDOOZeSJwjnnHMpeYJwzjmXkicI\n55xzKXmCcM45l5InCOeccyl5gnDOOZeSJwjnnHMpZTVBSBov6XVJKyXdkGafL0haIWm5pEdi6/dJ\nWhJNB4xE55xzLruyNqKcpCJgJjAOKAMWSppnZiti+wwEbgTGmtlmSR+LvcROMxuRrficc87VLJsl\niDHASjNbZWa7gTnAuUn7fAWYaWabAcxsQxbjcc45VwfZTBC9gXdiy2XRurijgaMl/U3SPySNj21r\nJ6k0Wn9eqjeQNDXap7S8vLxxo3fOuRYua1VMdXj/gcBJQB/gBUlDzWwL0M/M1kk6AnhO0r/N7K34\nwWY2C5gFYUzqpg3dOecKWzZLEOuAvrHlPtG6uDJgnpntMbPVwBuEhIGZrYv+rgKeB0ZmMVbnnHNJ\nspkgFgIDJQ2Q1AaYCCTfjfQEofSApJ6EKqdVkrpJahtbPxZYgXPOuSaTtSomM9sraRowHygC7jez\n5ZJmAKVmNi/adpqkFcA+4L/MrELSp4GfS9pPSGLfi9/95JxzLvtkVhhV9yUlJVZaWprrMJxzrlmR\ntMjMSlJt8yepnXPOpeQJwjnnXEqeIJxzzqXkCcI551xKniCcc86l5AnCOedcSp4gnHPOpeQJwjnn\nXEqeIJxzzqXkCcI551xKniCcc86l5AnCOedcSp4gnHPOpeQJwjnnXEqeIJxzzqWU1QQhabyk1yWt\nlHRDmn2+IGmFpOWSHomtv0TSm9F0STbjdM45d6CsjSgnqQiYCYwjjD29UNK8+MhwkgYCNwJjzWyz\npI9F67sDtwAlgAGLomM3Zyte55xz1WWzBDEGWGlmq8xsNzAHODdpn68AMxMXfjPbEK0/HXjazDZF\n254GxmcxVuecc0mymSB6A+/ElsuidXFHA0dL+pukf0gaX4djkTRVUqmk0vLy8kYM3TnnXK4bqQ8C\nBgInAZOA+yR1zfRgM5tlZiVmVtKrV68sheiccy1TNhPEOqBvbLlPtC6uDJhnZnvMbDXwBiFhZHKs\nc865LMpmglgIDJQ0QFIbYCIwL2mfJwilByT1JFQ5rQLmA6dJ6iapG3BatM4551wTydpdTGa2V9I0\nwoW9CLjfzJZLmgGUmtk8qhLBCmAf8F9mVgEg6duEJAMww8w2ZStW55xzB5KZ5TqGRlFSUmKlpaW5\nDsM555oVSYvMrCTVtlw3UjvnnMtTniCcc86l5AnCOedcSp4gnHPOpeQJwjnnXEqeIJxzzqXkCcI5\n51xKniCcc86l1OITxOzZ0L8/tGoV/s6eneuInHMuP2Stq43mYPZsmDoVduwIy2vXhmWAyZNzF5dz\nzuWDFl2CmD69Kjkk7NgR1jvnXEvXohPE22/Xbb1zzrUkLTpBHH543dY751xL0qITxG23QYcO1dd1\n6BDWO+dcS9eiE8TkyTBrFvTrB1L4O2uWN1A75xxkOUFIGi/pdUkrJd2QYvsUSeWSlkTTZbFt+2Lr\nk0eiazSTJ8OaNbB/f/jrycE554Ks3eYqqQiYCYwjjD29UNI8M1uRtOuvzWxaipfYaWYjshWfc865\nmmWzBDEGWGlmq8xsNzAHODeL7+ecc64RZTNB9AbeiS2XReuSnS9pmaTHJPWNrW8nqVTSPySdl8U4\nnXPOpZDrRurfA/3NbBjwNPBgbFu/aJzUi4F7JB2ZfLCkqVESKS0vL2+aiJ1zroXIZoJYB8RLBH2i\ndZXMrMLMdkWLvwBGx7ati/6uAp4HRia/gZnNMrMSMyvp1atX40bvnHMtXDb7YloIDJQ0gJAYJhJK\nA5UkHWZm66PFc4BXo/XdgB1mtktST2AscEdNb7Zo0aKNktbWsEtPYGO9Pkn2eWz147HVj8dWP4Ua\nW790G7KWIMxsr6RpwHygCLjfzJZLmgGUmtk84GuSzgH2ApuAKdHhxwA/l7SfUMr5Xoq7n5Lfr8Yi\nhKTSqMoq73hs9eOx1Y/HVj8tMbas9uZqZk8BTyWtuzk2fyNwY4rj/g4MzWZszjnnapbrRmrnnHN5\nqiUliFm5DqAGHlv9eGz147HVT4uLTWaWjdd1zjnXzLWkEoRzzrk68AThnHMupYJPELX1KJtrktZI\n+nfUa21pjmO5X9IGSa/E1nWX9LSkN6O/3fIotlslrYv1+ntmDuLqK2mBpBWSlku6Olqf8/NWQ2z5\ncN7aSfqXpKVRbN+K1g+Q9M/o/+uvJbXJo9gekLQ6dt5y1pmopCJJiyU9GS1n57yZWcFOhOcv3gKO\nANoAS4HiXMeVFOMaoGeu44hiOQEYBbwSW3cHcEM0fwPw/TyK7VbgGzk+Z4cBo6L5zsAbQHE+nLca\nYsuH8yagUzTfGvgncBwwF5gYrf8ZcEUexfYAcEEuz1ssxmuBR4Ano+WsnLdCL0F4j7J1YGYvEB5Y\njDuXqj6yHgRy0nFimthyzszWm9nL0fxWQm8AvcmD81ZDbDlnwbZosXU0GXAK8Fi0PlfnLV1seUFS\nH+AsQvdESBJZOm+FniAy7VE2lwz4s6RFkqbmOpgUDrGq7lDeAw7JZTApTIt6A74/V9VfCZL6E/oM\n+yd5dt6SYoM8OG9RNckSYAOhs863gC1mtjfaJWf/X5NjM7PEebstOm93S2qbi9iAe4Drgf3Rcg+y\ndN4KPUE0B8eb2SjgDOBKSSfkOqB0LJRf8+aXFPBT4EhgBLAeuCtXgUjqBPwW+LqZfRjfluvzliK2\nvDhvZrbPwqBgfQil/UG5iCOV5NgkDSH0+jAI+CTQHfjvpo5L0tnABjNb1BTvV+gJotYeZXPNqnqt\n3QA8TviPkk/el3QYhM4VCb+o8oKZvR/9R94P3EeOzp2k1oQL8Gwz+99odV6ct1Sx5ct5SzCzLcAC\n4FNAV0mJLoBy/v81Ftv4qMrOLPRA/Utyc97GAudIWkOoMj8FuJcsnbdCTxCVPcpGrfoTgayNb11X\nkjpK6pyYB04DXqn5qCY3D7gkmr8E+F0OY6kmcQGOfJ4cnLuo/vf/Aa+a2f/ENuX8vKWLLU/OWy9J\nXaP59oShiV8lXIwviHbL1XlLFdtrsYQvQh1/k583M7vRzPqYWX/C9ew5M5tMts5brlvjsz0BZxLu\n3ngLmJ7reJJiO4JwZ9VSYHmu4wMeJVQ57CHUY36ZUL/5LPAm8AzQPY9iewj4N7CMcEE+LAdxHU+o\nPloGLImmM/PhvNUQWz6ct2HA4iiGV4Cbo/VHAP8CVgK/AdrmUWzPReftFeBhojudcjUBJ1F1F1NW\nzpt3teGccy6lQq9ics45V0+eIJxzzqXkCcI551xKniCcc86l5AnCOedcSp4gnKuFpH2xHjyXqBF7\nBZbUP95DrXP55KDad3GuxdtpodsF51oUL0E4V08KY3ncoTCex78kHRWt7y/puahTt2clHR6tP0TS\n49E4A0slfTp6qSJJ90VjD/w5enoXSV+LxnJYJmlOjj6ma8E8QThXu/ZJVUwXxbZ9YGZDgR8TetkE\n+BHwoJkNA2YDP4zW/xD4i5kNJ4xtsTxaPxCYaWaDgS3A+dH6G4CR0etcnq0P51w6/iS1c7WQtM3M\nOqVYvwY4xcxWRZ3ivWdmPSRtJHRfsSdav97MekoqB/pY6Owt8Rr9Cd1JD4yW/xtobWbfkfQnYBvw\nBPCEVY1R4FyT8BKEcw1jaebrYldsfh9VbYNnATMJpY2Fsd46nWsSniCca5iLYn9fiub/TuhpE2Ay\n8Ndo/lngCqgckKZLuheV1Aroa2YLCOMOdAEOKMU4l03+i8S52rWPRhdL+JOZJW517SZpGaEUMCla\ndxXwS0n/BZQDX4rWXw3MkvRlQknhCkIPtakUAQ9HSUTADy2MTeBck/E2COfqKWqDKDGzjbmOxbls\n8Com55xzKXkJwjnnXEpegnDOOZeSJwjnnHMpeYJwzjmXkicI55xzKXmCcM45l9L/BwIChT5lbEIL\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FBpTc_rXGvQ"
      },
      "source": [
        "The accuracy of model2 is 87%. Using Embedding layer instead of one-hot layer improved the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "--020hfG6rN2"
      },
      "source": [
        "### Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J4gBeOyi4gkM"
      },
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOsgGRXg77cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install chakin\n",
        "# import chakin\n",
        "# chakin.download(number=12, save_dir='./')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_PypdqG9Iis",
        "colab": {}
      },
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZcIZ3dq59bCh"
      },
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gembn7VM3ex8",
        "colab": {}
      },
      "source": [
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
        "    return embeddingLayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HGxciLK4-xOr"
      },
      "source": [
        "We freeze the weights. To create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZCPUM0W_Drc",
        "colab": {}
      },
      "source": [
        "# put the code here\n",
        "import os\n",
        "# os.system(\"unzip './glove.6B.zip' \")\n",
        "from tensorflow.contrib.keras.api.keras.initializers import Constant\n",
        "wordToIndex, indexToWord, wordToGlove = readGloveFile('./glove.6B.300d.txt')\n",
        "embeddingLayer = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable=True)\n",
        "# os.system(\"rm './glove.6B.zip'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-bZ5SCHiIMl"
      },
      "source": [
        "### Adding another hidden layer to the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbZ6UBDfbjea"
      },
      "source": [
        "In model3, we only add another dense layer to see if that improves the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vw0le1YjDdCa",
        "outputId": "e7f3e13d-7ba0-429a-9024-b90850187f2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "# put your code here\n",
        "model3 = Sequential()\n",
        "# model3.add(Embedding(VOCAB_SIZE,16,input_length=MAX_SEQUENCE_LENGTH))\n",
        "model3.add(embeddingLayer)\n",
        "model3.add(GlobalAveragePooling1DMasked())\n",
        "model3.add(Dense(16,activation='relu'))\n",
        "model3.add(Dense(16,activation='relu'))\n",
        "model3.add(Dense(1,activation='sigmoid'))\n",
        "model3.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 300)         120000300 \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 120,005,405\n",
            "Trainable params: 120,005,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgiSSc-dI_05",
        "colab_type": "code",
        "outputId": "43862e29-d598-4392-f818-10a8ad8e497c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history3 = model3.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model3.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:421: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120000300 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 5s 319us/step - loss: 0.6435 - acc: 0.8477 - val_loss: 0.5960 - val_acc: 0.8308\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.4898 - acc: 0.9431 - val_loss: 0.4450 - val_acc: 0.8626\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.2964 - acc: 0.9690 - val_loss: 0.3301 - val_acc: 0.8753\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.1720 - acc: 0.9777 - val_loss: 0.3028 - val_acc: 0.8784\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.1163 - acc: 0.9810 - val_loss: 0.3092 - val_acc: 0.8776\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.0864 - acc: 0.9868 - val_loss: 0.3267 - val_acc: 0.8769\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0673 - acc: 0.9895 - val_loss: 0.3522 - val_acc: 0.8732\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0551 - acc: 0.9912 - val_loss: 0.3762 - val_acc: 0.8720\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.0456 - acc: 0.9927 - val_loss: 0.3981 - val_acc: 0.8719\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.0373 - acc: 0.9957 - val_loss: 0.4205 - val_acc: 0.8728\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.0315 - acc: 0.9960 - val_loss: 0.4487 - val_acc: 0.8687\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 3s 192us/step - loss: 0.0268 - acc: 0.9970 - val_loss: 0.4684 - val_acc: 0.8691\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0231 - acc: 0.9975 - val_loss: 0.4918 - val_acc: 0.8672\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.0202 - acc: 0.9976 - val_loss: 0.5143 - val_acc: 0.8665\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0174 - acc: 0.9984 - val_loss: 0.5368 - val_acc: 0.8654\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0149 - acc: 0.9988 - val_loss: 0.5558 - val_acc: 0.8656\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.0131 - acc: 0.9988 - val_loss: 0.5789 - val_acc: 0.8632\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.0119 - acc: 0.9989 - val_loss: 0.6045 - val_acc: 0.8635\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.0104 - acc: 0.9992 - val_loss: 0.6178 - val_acc: 0.8638\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.0090 - acc: 0.9991 - val_loss: 0.6384 - val_acc: 0.8636\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.0077 - acc: 0.9993 - val_loss: 0.6575 - val_acc: 0.8629\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0071 - acc: 0.9996 - val_loss: 0.6708 - val_acc: 0.8618\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.0060 - acc: 0.9995 - val_loss: 0.6884 - val_acc: 0.8621\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0052 - acc: 0.9998 - val_loss: 0.7032 - val_acc: 0.8617\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0047 - acc: 0.9998 - val_loss: 0.7179 - val_acc: 0.8614\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0042 - acc: 0.9998 - val_loss: 0.7323 - val_acc: 0.8619\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.0039 - acc: 0.9998 - val_loss: 0.7506 - val_acc: 0.8618\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0034 - acc: 0.9999 - val_loss: 0.7590 - val_acc: 0.8619\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.7728 - val_acc: 0.8616\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 3s 192us/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.7864 - val_acc: 0.8615\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.7961 - val_acc: 0.8618\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 0.8082 - val_acc: 0.8623\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.8194 - val_acc: 0.8607\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0020 - acc: 0.9999 - val_loss: 0.8294 - val_acc: 0.8611\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.8396 - val_acc: 0.8613\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.8505 - val_acc: 0.8610\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.8614 - val_acc: 0.8610\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.8685 - val_acc: 0.8613\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.8807 - val_acc: 0.8609\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 3s 193us/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.8877 - val_acc: 0.8608\n",
            "25000/25000 [==============================] - 1s 41us/step\n",
            "[0.946351080160141, 0.84748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vSN29uCKQ_L",
        "colab_type": "code",
        "outputId": "4fe9a499-25c7-4887-cbcd-853d12161517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "results = model3.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 43us/step\n",
            "[0.946351080160141, 0.84748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QtsdVeW7UgCu",
        "outputId": "91668541-8f5d-4080-a317-35f00e103aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history3.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD6TRdf84+MjQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx--Ytk3ZbLo"
      },
      "source": [
        "The accuracy of model3 with an additional layer is 85%. Adding more layers can help you to extract more features. But we can do that upto a certain extent. After some point, instead of extracting features, we tend to overfit the data. Overfitting can lead to errors in some or the other form like false positives. It is not easy to choose the number of units in a hidden layer or the number of hidden layers in a neural network. For many applications, one hidden layer is enough. As a general rule, the number of units in that hidden layer is between the number of inputs and the number of outputs.\n",
        " The best way to decide on the number of units and hidden layers is to try various parameters. Train several neural networks with different numbers of hidden layers and neurons, and monitor the performance of them. You will have to experiment using a series of different architectures. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gn2GSV4ioyO2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XYC6DykEox2w",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GsCJ01StlgCx"
      },
      "source": [
        "This tutorial is substantially based on this document:\n",
        "https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
        "\n",
        "To read more about Sequential APIs you can go to: https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "The one-hot word vector layer is taken from:\n",
        "https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jL0UovfaE9GE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}