{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Convolution1D, MaxPooling1D, MaxPooling2D, Convolution2D, LSTM\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
    "from keras.regularizers import l2, l1\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import one_hot as oneHOT\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_tweet = pd.read_csv(\"data/processed_tweet2.csv\")\n",
    "import ast\n",
    "df_tweet['tweet_pr_st1'] = df_tweet.apply(lambda x: ast.literal_eval(x.tweet_pr_st1),axis =1)\n",
    "df_tweet['tweet_pr_st2'] = df_tweet.apply(lambda x: ast.literal_eval(x.tweet_pr_st2),axis =1)\n",
    "\n",
    "df_tweet['subtask_a'] = df_tweet.apply(lambda x: 0 if x.subtask_a=='NOT' else 1,axis = 1)\n",
    "df_tweet['subtask_b'] = df_tweet.apply(lambda x: 1 if x.subtask_b=='UNT' else 2 if x.subtask_b=='TIN' else 0,axis = 1)\n",
    "df_tweet['subtask_c'] = df_tweet.apply(lambda x: 2 if x.subtask_c=='IND' else 3 if x.subtask_c=='GRP' else 4 if x.subtask_c =='OTH' else 0,axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_corpus = list(df_tweet['tweet_pr_st2'])\n",
    "labels = list(df_tweet['subtask_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ask', 'native', 'american', 'take'],\n",
       " ['go',\n",
       "  'home',\n",
       "  'drunk',\n",
       "  'maga',\n",
       "  'trump',\n",
       "  'oncoming',\n",
       "  'fist',\n",
       "  'united',\n",
       "  'state',\n",
       "  'oncoming',\n",
       "  'fist'],\n",
       " ['amazon',\n",
       "  'investigate',\n",
       "  'chinese',\n",
       "  'employee',\n",
       "  'sell',\n",
       "  'internal',\n",
       "  'data',\n",
       "  'third',\n",
       "  'party',\n",
       "  'seller',\n",
       "  'look',\n",
       "  'edge',\n",
       "  'competitive',\n",
       "  'marketplace',\n",
       "  'amazon',\n",
       "  'maga',\n",
       "  'kag',\n",
       "  'china',\n",
       "  'tcot']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalised_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, itertools\n",
    "word_counter = collections.Counter(list(itertools.chain.from_iterable(normalised_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13910"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = list(set(list(itertools.chain.from_iterable(normalised_corpus))))\n",
    "len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "word2idx = {token:id+3 for id,token in enumerate(word_ids)}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<START>'] = 1\n",
    "word2idx['<UNK>'] = 2\n",
    "word2idx['<UNUSED>'] = 3\n",
    "\n",
    "# word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "# word_index[\"<PAD>\"] = 0\n",
    "# word_index[\"<START>\"] = 1\n",
    "# word_index[\"<UNK>\"] = 2  \n",
    "# word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v: k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_as_ids = []\n",
    "for i in range(len(normalised_corpus)):\n",
    "    sents_as_ids.append([word2idx[j] for j in normalised_corpus[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ask', 'native', 'american', 'take'],\n",
       " ['go',\n",
       "  'home',\n",
       "  'drunk',\n",
       "  'maga',\n",
       "  'trump',\n",
       "  'oncoming',\n",
       "  'fist',\n",
       "  'united',\n",
       "  'state',\n",
       "  'oncoming',\n",
       "  'fist'],\n",
       " ['amazon',\n",
       "  'investigate',\n",
       "  'chinese',\n",
       "  'employee',\n",
       "  'sell',\n",
       "  'internal',\n",
       "  'data',\n",
       "  'third',\n",
       "  'party',\n",
       "  'seller',\n",
       "  'look',\n",
       "  'edge',\n",
       "  'competitive',\n",
       "  'marketplace',\n",
       "  'amazon',\n",
       "  'maga',\n",
       "  'kag',\n",
       "  'china',\n",
       "  'tcot']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalised_corpus[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6980, 10805, 5741, 6546],\n",
       " [5262, 13760, 2911, 7884, 3089, 8992, 582, 9484, 7373, 8992, 582],\n",
       " [2956,\n",
       "  2959,\n",
       "  10462,\n",
       "  6329,\n",
       "  2507,\n",
       "  8689,\n",
       "  13136,\n",
       "  9394,\n",
       "  12842,\n",
       "  9038,\n",
       "  11515,\n",
       "  13229,\n",
       "  1089,\n",
       "  998,\n",
       "  2956,\n",
       "  7884,\n",
       "  3648,\n",
       "  6382,\n",
       "  11126]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_as_ids[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_ids) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nv/virtpy3tf/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# def one_hot(labels):\n",
    "#     from sklearn.preprocessing import OneHotEncoder\n",
    "#     encoder = OneHotEncoder()\n",
    "#     return encoder.fit_transform(np.array(labels).reshape(-1,1)).toarray()\n",
    "\n",
    "# labels = one_hot(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGloveFile(gloveFile):\n",
    "    with open(gloveFile, 'r') as f:\n",
    "        wordToGlove = {}  \n",
    "        wordToIndex = {}  \n",
    "        indexToWord = {}  \n",
    "\n",
    "        for line in f:\n",
    "            record = line.strip().split()\n",
    "            token = record[0] \n",
    "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
    "            \n",
    "        tokens = sorted(wordToGlove.keys())\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            kerasIdx = idx + 1  \n",
    "            wordToIndex[tok] = kerasIdx \n",
    "            indexToWord[kerasIdx] = tok \n",
    "\n",
    "    return wordToIndex, indexToWord, wordToGlove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
    "    vocabLen = len(wordToIndex) + 1  \n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
    "   \n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] \n",
    "\n",
    "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
    "    return embeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_labels,test_labels = train_test_split(sents_as_ids,labels,test_size=0.2,stratify=labels)\n",
    "\n",
    "X_train_enc = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=50)\n",
    "X_test_enc = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chakin in /home/nv/virtpy3tf/lib/python3.6/site-packages (0.0.8)\n",
      "Requirement already satisfied: pandas>=0.20.1 in /home/nv/virtpy3tf/lib/python3.6/site-packages (from chakin) (0.25.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/nv/virtpy3tf/lib/python3.6/site-packages (from chakin) (1.12.0)\n",
      "Requirement already satisfied: progressbar2>=3.20.0 in /home/nv/virtpy3tf/lib/python3.6/site-packages (from chakin) (3.47.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/nv/virtpy3tf/lib/python3.6/site-packages (from pandas>=0.20.1->chakin) (1.16.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/nv/virtpy3tf/lib/python3.6/site-packages (from pandas>=0.20.1->chakin) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/nv/virtpy3tf/lib/python3.6/site-packages (from pandas>=0.20.1->chakin) (2.8.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /home/nv/virtpy3tf/lib/python3.6/site-packages (from progressbar2>=3.20.0->chakin) (2.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# put the code here\n",
    "# !pip install chakin\n",
    "# import chakin\n",
    "# chakin.download(number=12, save_dir='embeddings/')\n",
    "\n",
    "# import os\n",
    "# os.system(\"unzip 'embeddings/glove.6B.zip' \")\n",
    "from tensorflow.contrib.keras.api.keras.initializers import Constant\n",
    "wordToIndex, indexToWord, wordToGlove = readGloveFile('embeddings/glove.6B.300d.txt')\n",
    "embeddingLayer = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable=True)\n",
    "# os.system(\"rm 'embeddings/glove.6B.zip'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# MAXIMUM_LENGTH = 500\n",
    "# train_data,test_data,train_labels,test_labels = train_test_split(sents_as_ids,labels,test_size=0.2,stratify=labels)\n",
    "\n",
    "# preprocessed_train_data = pad_sequences(train_data,maxlen=MAXIMUM_LENGTH)\n",
    "# processed_test_data = pad_sequences(test_data,maxlen=MAXIMUM_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sample train_data before preprocessing: 27\n"
     ]
    }
   ],
   "source": [
    "# print('Length of sample train_data before preprocessing:', len(train_data[1]), type(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sample train_data after preprocessing: 500\n"
     ]
    }
   ],
   "source": [
    "# print('Length of sample train_data after preprocessing:', len(preprocessed_train_data[1]), type(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         120000300 \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 120,160,801\n",
      "Trainable params: 120,160,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "EMBED_SIZE = 100\n",
    "model.add(embeddingLayer)\n",
    "# model.add(Embedding(vocab_size,EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(1,activation='sigmoid',input_shape=(1,)))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# put the code here\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 30, input_length=MAXIMUM_LENGTH))\n",
    "# model.add(Convolution1D(64,5,activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Convolution1D(32,3,activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Convolution1D(16,3,activation=\"sigmoid\"))\n",
    "# model.add(MaxPooling1D(5))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(train_labels.shape[1],activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# X_val = np.array(X_train_enc[:10000])\n",
    "# partial_X_train = np.array(X_train_enc[10000:])\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=15,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.08,\n",
    "                    verbose=1)\n",
    "\n",
    "results = model.evaluate(X_test_enc, y_test)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
